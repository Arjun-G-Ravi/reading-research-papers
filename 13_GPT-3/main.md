# Language Models are Few-Shot Learners
- OpenAI

- Introduces the concept of few shot learning, over fine-tuning(which requires training)

- Larger models make increasingly efficient use of in-context information. They can make better use of data in their context(like few shot data)
- Introduces GPT-3, with 8 different models varying from 125M to 175B parameters - none of them are open source
- Has the same architecture as GPT-2
- The dataset is super cleaned
- Larger models can typically use a larger batch size, but require a smaller learning rate.

The paper then disctusss the performance, result on various benchmark, safety concerns, biases, etc.
