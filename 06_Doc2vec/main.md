# Distributed Representations of Sentences and Documents

## Google
- The conventional way of representing text as a fixed length data (to be fed as input for ML models) is through bag of words. This has no positional encoding. Also it doesn't learn the real meaning of a word, as all words are equidistant when represented in space.
- This paper introduce embeddings
- Each document is represented by a dense vector which is trained to predict words in the document.     
- 