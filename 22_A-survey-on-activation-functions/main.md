# Activation Functions in Deep Learning: A Comprehensive Survey and Benchmark

Shiv Ram Dubey1, Satish Kumar Singh1, Bidyut Baran Chaudhur

- the main goal of any neural network is to transform
the non-linearly separable input data into more linearly separable abstract features using a hierarchy of layers.
- normalising and standarising is very important for most AFs
- Despite the ReLU being a popular choice, recently pro-
posed AFs such as Swish, Mish, and PAU are also worth
trying for different problems.
- The ReLU, LReLU, ELU, GELU, CELU, and PDELU
functions are better for the networks having residual con-
nections for image classiﬁcation.

• The exponential AFs generally lead to the increased non-
linearity due to utilization of the negative values.

• The Tanh and SELU AFs are found better for language
translation along with PReLU, LiSHT, SRS and PAU.

• It is suggested to use the PReLU, GELU, Swish, Mish and
PAU AFs for speech recognition.

- 